{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import trafficDataUtility as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\KangYounKook\\\\Desktop\\\\ML_Study\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"sectionInfo.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getDataFromFile  C:\\Users\\KangYounKook\\Desktop\\ML_Study\\sectionInfo.txt\n"
     ]
    }
   ],
   "source": [
    "dataset = {}\n",
    "dataset = util.getDatasetFromFile(path+\"sectionInfo.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadDataFromFile  C:\\Users\\KangYounKook\\Desktop\\ML_Study\\PeriodTotalAverage.txt\n",
      "update Dataset [move_count,work_count,ratio]\n"
     ]
    }
   ],
   "source": [
    "filename = \"PeriodTotalAverage.txt\"\n",
    "util.updateDatasetFromFile(path+filename, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadCsvInput  C:\\Users\\KangYounKook\\Desktop\\ML_Study\\input10_all.csv\n"
     ]
    }
   ],
   "source": [
    "filename = \"input\" + str(classes) +\"_all.csv\"\n",
    "#filename = \"input10_all.csv\"\n",
    "data = util.loadCsvInput(path,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outfilename = \"DNN_Result\" + str(classes) + \".txt\"\n",
    "f = open(outfilename,'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#selectedList = dataset.keys()\n",
    "selectedList = ['Section697','Section754','Section1212','Section852']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkCondition(id,data,dataset):\n",
    "    if id not in data.columns:\n",
    "        return False\n",
    "    if dataset[id][5] < 1440:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Section697 -------------------------\n",
      "Training: 0.763\n",
      "Validation: 0.789474\n",
      "2 Section754 -------------------------\n",
      "Training: 0.752\n",
      "Validation: 0.75286\n",
      "3 Section1212 -------------------------\n",
      "Training: 0.67\n",
      "Validation: 0.675057\n",
      "4 Section852 -------------------------\n",
      "Training: 0.771\n",
      "Validation: 0.775744\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for id in selectedList:\n",
    "        \n",
    "    if checkCondition(id,data,dataset) == False:\n",
    "        continue\n",
    "        \n",
    "    Xa = util.getX(id,data,dataset)\n",
    "    if Xa is None:\n",
    "#        print('X is none')\n",
    "        continue\n",
    "    Ya = np.array(data[id]).reshape(-1,1)\n",
    "#    print(Ya.shape)\n",
    "\n",
    "    num += 1\n",
    "    print(num, id, '-------------------------')\n",
    "    Xn = np.c_[Xa[:1438],Xa[1:1439]]\n",
    "#    Xn = np.c_[Xa[:1437],Xa[1:1438],Xa[2:1439]]\n",
    "#    print(Xn.shape)\n",
    "    Yn = Ya[2:1440]\n",
    "#    Yn = Ya[3:1440]\n",
    "#    print(Yn.shape)\n",
    "    \n",
    "    Xt = Xn[0:1000]\n",
    "    Xval = Xn[1000:1437]\n",
    "    Yt = Yn[0:1000]\n",
    "    Yval = Yn[1000:1437]\n",
    "\n",
    "    input_size = 10    \n",
    "#    input_size = 15\n",
    "#    classes = 5\n",
    "    X = tf.placeholder(tf.float32, [None, input_size])\n",
    "    Y = tf.placeholder(tf.int32, [None, 1])\n",
    "    Y_one_hot = tf.one_hot(Y, classes)\n",
    "    Y_one_hot = tf.reshape(Y_one_hot, [-1, classes])\n",
    "\n",
    "    W1 = tf.Variable(tf.random_normal([input_size, 7]))\n",
    "    #W1 = tf.get_variable(\"W1\", shape=[5,classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([7]))\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "    W2 = tf.Variable(tf.random_normal([7, 7]))\n",
    "    #W2 = tf.get_variable(\"W2\", shape=[5,5], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([7]))\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "    W3 = tf.Variable(tf.random_normal([7, classes]))\n",
    "    #W3 = tf.get_variable(\"W3\", shape=[5,5], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.Variable(tf.random_normal([classes]))\n",
    "    hypothesis = tf.matmul(L2, W3) + b3\n",
    "    \n",
    "    cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis,labels=Y_one_hot)\n",
    "    cost = tf.reduce_mean(cost_i)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.1).minimize(cost)\n",
    "    prediction = tf.argmax(hypothesis, 1)\n",
    "    correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(100):\n",
    "        c, _ = sess.run([cost,optimizer], feed_dict={X:Xt,Y:Yt})\n",
    "    \n",
    "    trainAccuracy = sess.run(accuracy, feed_dict={X:Xt,Y:Yt})\n",
    "    valAccuracy = sess.run(accuracy, feed_dict={X:Xval,Y:Yval})    \n",
    "    print('Training:', trainAccuracy)\n",
    "    print('Validation:', valAccuracy)\n",
    "\n",
    "    log = id\n",
    "    log += ',' + dataset[id][0]\n",
    "    log += ',' + dataset[id][1]\n",
    "    log += ',' + str(dataset[id][5])\n",
    "    log += ',' + str(dataset[id][7])\n",
    "    log += ',' + str(trainAccuracy)\n",
    "    log += ',' + str(valAccuracy)\n",
    "    log += '\\n'\n",
    "    f.write(log)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
